{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZ88HxNEA9r0"
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "This notebook is a cleaned version of the Word2Vect practical, adapted to a French dataset, in order to get usable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTFsOQ8eA9r6"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4XKQvI3A9sD"
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc5 in position 6: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8d4e55aea652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'wikipediaTXT.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# st is the abbreviation of \"sentence\" in the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mraw_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc5 in position 6: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "ROOT_DIR='content'\n",
    "ROOT_DIR = Path.home()\n",
    "data_path = os.path.join(ROOT_DIR,'Desktop/NLProj/data/')\n",
    "file = 'wikipediaTXT.txt'\n",
    "with open(data_path+file, 'r',encoding='ISO-8859-1') as f:\n",
    "    lines = f.readlines()\n",
    "    # st is the abbreviation of \"sentence\" in the loop\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "\n",
    "'# sentences: %d' % len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0PQHu8NA9sW"
   },
   "outputs": [],
   "source": [
    "# tk is an abbreviation for \"token\" in the loop\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcqXd38FA9sm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 887100'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in raw_dataset]\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "'# tokens: %d' % num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAz0GiAYA9s7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# tokens: 375392'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "'# tokens: %d' % sum([len(st) for st in subsampled_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jv1nPR-NA9tA"
   },
   "outputs": [],
   "source": [
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (token, sum(\n",
    "        [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
    "        [st.count(token_to_idx[token]) for st in subsampled_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8huGW8DAA9tK"
   },
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        # Each sentence needs at least 2 words to form a\n",
    "        # \"central target word - context word\" pair\n",
    "        if len(st) < 2:\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            # Exclude the central target word from the context words\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2k9PtoqA9tZ"
   },
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pfd8ukxhA9tl"
   },
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                # An index of k words is randomly generated as noise words\n",
    "                # based on the weight of each word (sampling_weights). For\n",
    "                # efficient calculation, k can be set slightly larger\n",
    "                i, neg_candidates = 0, random.choices(population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            # Noise words cannot be context words\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJs9ymetA9t3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t0GFenm_A9t7"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuheXdUEA9t_"
   },
   "outputs": [],
   "source": [
    "class PTB_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, all_centers, all_contexts, all_negatives):\n",
    "        self.all_centers, self.all_contexts_negatives, self.all_masks, self.all_labels = self.batchify(list(zip(all_centers,all_contexts,all_negatives)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.all_centers)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.all_centers[idx], self.all_contexts_negatives[idx], self.all_masks[idx], self.all_labels[idx]\n",
    "        \n",
    "    def batchify(self,data):\n",
    "        max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "        centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "        for center, context, negative in data:\n",
    "            cur_len = len(context) + len(negative)\n",
    "            centers += [center]\n",
    "            contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "            masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "            labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "        return (torch.tensor(centers).view((-1, 1)), torch.tensor(np.array(contexts_negatives)),\n",
    "            torch.tensor(np.array(masks)), torch.tensor(np.array(labels)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjAdHiQTA9uC"
   },
   "outputs": [],
   "source": [
    "ptbdata = PTB_dataset(all_centers, all_contexts, all_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMUFDG2LA9uI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "\n",
    "data_loader = DataLoader(ptbdata, batch_size, shuffle=True,\n",
    "                              num_workers=4)\n",
    "for batch in data_loader:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLz15ca5A9uM"
   },
   "outputs": [],
   "source": [
    "# taken from the spotlight library, see\n",
    "# https://github.com/maciejkula/spotlight/blob/master/spotlight/layers.py\n",
    "class ScaledEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    Embedding layer that initialises its values\n",
    "    to using a normal variable scaled by the inverse\n",
    "    of the emedding dimension.\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters.\n",
    "        \"\"\"\n",
    "        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n",
    "        if self.padding_idx is not None:\n",
    "            self.weight.data[self.padding_idx].fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTOeSlqmA9uP"
   },
   "outputs": [],
   "source": [
    "class Skip_gram(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size = 100):\n",
    "        super(Skip_gram, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_size = embed_size\n",
    "        self.central_emb = ScaledEmbedding(self.input_dim,self.embed_size)\n",
    "        self.context_emb = ScaledEmbedding(self.input_dim,self.embed_size)\n",
    "        \n",
    "    def forward(self, icent, icont):\n",
    "        # (hint: dimensions are for icent (bs,1) for icont (bs,max_len) and output (bs,1,max_len))\n",
    "        output = torch.einsum('aij,akj->aik', self.central_emb(icent), self.context_emb(icont))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0Z7NPX1A9uT"
   },
   "outputs": [],
   "source": [
    "net = Skip_gram(len(idx_to_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cb2V_wIEA9uY"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "def criterion(pred, label, mask):\n",
    "    #\n",
    "    return [loss_fn(p, l.type_as(p)) * m for p, l, m in zip(pred, label, mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfx1o6c1A9ud"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WONDLY8IA9uf"
   },
   "outputs": [],
   "source": [
    "def train(n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start, loss = time.time(), 0\n",
    "        print(len(data_loader))\n",
    "        for batch in data_loader:\n",
    "            #\n",
    "            cent, cont, mask, label = batch\n",
    "            pred = torch.squeeze(net(cent, cont))\n",
    "            loss = torch.sum(sum(criterion(pred, label, mask)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, loss, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCSPXuRjA9ug",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732\n",
      "epoch 1, loss 2001.61, time 177.00s\n",
      "732\n",
      "epoch 2, loss 1889.13, time 170.48s\n",
      "732\n",
      "epoch 3, loss 1886.85, time 180.14s\n",
      "732\n",
      "epoch 4, loss 1813.41, time 170.84s\n",
      "732\n",
      "epoch 5, loss 1725.01, time 166.89s\n",
      "732\n",
      "epoch 6, loss 1662.56, time 171.37s\n"
     ]
    }
   ],
   "source": [
    "train(6)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "Fux1T7u-A9uj"
   },
   "source": [
    "## Applying the Word Embedding Model\n",
    "\n",
    "After training the word embedding model, we can represent similarity in meaning between words based on the cosine similarity of two word vectors. As we can see, when using the trained word embedding model, the words closest in meaning to the word \"chip\" are mostly related to chips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPQZOcdGA9uk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.516: intel\n",
      "cosine sim=0.438: screens\n",
      "cosine sim=0.430: microprocessor\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, W):\n",
    "    #\n",
    "    # your code here\n",
    "    #\n",
    "    token_id = token_to_idx[query_token]\n",
    "    cos_sim = nn.CosineSimilarity()\n",
    "    cos = torch.Tensor([cos_sim(net.central_emb(torch.LongTensor([token_id])), net.central_emb(torch.LongTensor([i]))) for i in range(len(idx_to_token))])\n",
    "    \n",
    "    _,topk = torch.topk(cos, k=k+1,)\n",
    "    for i in topk[1:]:# Remove the input words\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('chip', 3, net.central_emb.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = [net.central_emb(torch.LongTensor([i])).tolist() for i in range(len(idx_to_token))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'embs' (list)\n"
     ]
    }
   ],
   "source": [
    "%store embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'token_to_idx' (dict)\n",
      "Stored 'idx_to_token' (list)\n"
     ]
    }
   ],
   "source": [
    "%store token_to_idx\n",
    "%store idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": false,
   "name": "08_Word2vec_pytorch_empty.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
